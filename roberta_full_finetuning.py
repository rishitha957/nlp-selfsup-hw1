# -*- coding: utf-8 -*-
"""roberta_full_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1okiOde35eZovD4CJTERS9kk9ptVtTQe_
"""

import torch
from torch import cuda
from datasets import load_dataset
from transformers import AutoTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from transformers import TrainerCallback

device = 'cuda' if cuda.is_available() else 'cpu'

print(device)

tokenizer = AutoTokenizer.from_pretrained("roberta-base")
model = RobertaForSequenceClassification.from_pretrained("roberta-base").to(device)

ds = load_dataset("stanfordnlp/sst2")


train_ds = ds['train']
test_ds = ds['test']
val_ds = ds['validation']

print(len(train_ds))
print(len(test_ds))
print(len(val_ds))

def tokenize(batch):
    return tokenizer(batch['sentence'], padding=True, truncation=True)

train_ds = train_ds.map(tokenize, batched=True, batch_size=len(train_ds))
test_ds = test_ds.map(tokenize, batched=True, batch_size=len(test_ds))
val_ds = val_ds.map(tokenize, batched=True, batch_size=len(val_ds))

train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

class TrainAccCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        # Perform a prediction on the training set
        train_predictions = trainer.predict(train_ds)
        train_preds = train_predictions.predictions.argmax(axis=-1)
        train_labels = train_predictions.label_ids
        train_accuracy = accuracy_score(train_labels, train_preds)
        print(f"Training Accuracy after epoch {state.epoch}: {train_accuracy:.4f}")

training_args = {
    "output_dir": "./output",
    "overwrite_output_dir": True,
    "num_train_epochs": 4,
    "per_device_train_batch_size": 16,
    "per_device_eval_batch_size": 16,
    "learning_rate": 1e-5,
    "weight_decay": 0.01,
    "load_best_model_at_end": True,
    "metric_for_best_model": "accuracy",
    "evaluation_strategy": "epoch",
    "save_strategy": "epoch",
    "logging_strategy": "epoch",
}

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return {"accuracy": accuracy_score(labels, predictions)}

training_args = TrainingArguments(**training_args)
trainer = Trainer(model=model, args=training_args, train_dataset=train_ds, eval_dataset=val_ds, compute_metrics=compute_metrics, callbacks=[TrainAccCallback()])

res = trainer.train()
trainer.save_model("./best_roberta_model")

train_logs = res.metrics
print(train_logs)

# Plotting the accuracy
plt.plot(train_logs['train_accuracy'], label='Train Accuracy')
plt.plot(train_logs['eval_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

test_ds = test_ds.map(tokenize, batched=True, batch_size=16)
test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_results = trainer.evaluate(eval_dataset=test_ds)
print(test_results)

